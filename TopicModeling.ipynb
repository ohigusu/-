{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysrC0j6TjXHg"
      },
      "source": [
        "# 1️⃣辞書定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ5A7FyJjWdr"
      },
      "source": [
        "## 権力構造と政治形態に関する単語のユーザー定義辞書\n",
        "北朝鮮には権力構造や政治形態を表す用語に韓国とは異なる点が多い。したがって、北朝鮮政府ポータルが提供する権力構造と政治形態に関する説明を参考にして、独自の用語辞書を作成する。\n",
        "\n",
        "参考サイト:https://nkinfo.unikorea.go.kr/nkp/pge/ps/jung.do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0IMcb9ljiXz"
      },
      "outputs": [],
      "source": [
        "PoliticalTerms = ['사회주의','조선노동당','수령','장군','우리식','우리 식','우리당','우리 당','우리민족','우리 민족','우리 힘','우리힘',\"우리나라\",'우리','로동당','조국','건설경제력', '경제적', '경제제재', '자립경제','국가경제',\n",
        "                      '조직지도부','선전선동부','간부부','경공업부','경제부','과학교육부','국제부','군수공업부','군정지도부','규율조사부','근로단체부','농업부','당역사연구소','조선로동당',\n",
        "                      '문서정리실','문화예술부','민방위부','법무부','재정경리부','총무부','10국','39호실','경제정책실','경제발전','경제사업','사회경제','경제구조','지방경제','경제활성화',\n",
        "                      '최고인민회의','국무위원회','내각','사법검찰기관','중앙재판소', '중앙검찰소', '인민정권','전원회의','령도','조선민주주의인민공화국','조선인민군','수령','리익',\n",
        "                      '혁명사상','로동계급','근로인민대중','핵심부대','전위부대','수령체제','인민대중','근로대중','중앙집권제','상의하달','조직생활','사상생활','조직지도부','선전선동부',\n",
        "                      '유일지배','이념','당무','향도자','최고지도자','전권','조선로동당대회','당 대회','당대회','당 대표자회','당대표자회','최고참모부','참모부'\n",
        "                      '의사결정','노선','정책',\"전략전술\",'통일','정치국','정치노선', '조직노선','조선로동당 위원장동지','화학','전력','교육','건설','나라','６.１５',\n",
        "                      '중앙지도기관','당중앙지도기관','중앙기관','당중앙기관','중앙검사위원회','당 중앙검사위원회','당중앙검사위원회','당중앙위원회','당 중앙위원회','중앙위원회',\n",
        "                      '상무위원','최고수위','위원장','총비서','비서','당대표자회',\"당 대표자회\",'대표자회', '당규약','인민경제발전', '정세', '과업','국방','경제','병진정책','경제건설','월남문제',\n",
        "                      '당 중앙위원장제','당중앙위원장제','중앙위원장제','직제','수령','제1비서직','중앙군사위원회','당중앙군사위원회','당 중앙군사위원회','최고지도기관','대행', '후보위원',\n",
        "                      '내외',\"논의\",'의결','권력기구','안건','확대회의','정무국','비서제','군사노선',\"공화국\",'무력','지휘','군수공업','국방사업','국방력','현대화','조직비서',\n",
        "                      '내각총리','주권기관','헌납운동','조선소년단','헌납','투쟁','계획','국방과학발전','국제정세','국위','국익','북남관계','대미대적','반공화국','대미','핵',\n",
        "                      '미국','북남','대적사업','조선반도','선박공업','국방공업','민방위무력','함선공업','무인항공공업','무인무장장비','무인','정찰위성','우주과학','미싸일','무장장비','대사변',\n",
        "                      '핵무기생산계획','남조선','핵위기사태','안보','무기체계개발계획','군사','인민군대','9.19북남군사분야합의','로골화','유엔군사령부','사회주의강성국가건설','인민군장병','조국통일위업','인민경제'\n",
        "                      '괴뢰패당','괴뢰군부','괴뢰군무력','괴뢰정권','망동','내각부총리','침략전쟁기도','군사분계선지역','농업위원회','대한민국','국가예산','예산','대회','윤석열',\n",
        "                      '조국통일로선','대북','통일','흡수통일','문재인','자유민주주의','민주주의','화성-18형','신형','고체연료','엔진','중거리','탄도미사일','중대과업','당 지방조직','지방조직',\n",
        "                      '집권체제','조선혁명','혁명','당 위원회','상하','위계','여타','기관','사회단체','지배력','행사','도,시,군','성,도,시,군','도,시,군인민회','도 당 위원회', '시 당 위원회', '군 당 위원회', '초급당', '분초급당', '부문당', '당원','최하','기층조직',\n",
        "                      '당세포','관할지역','중앙당','하부','조직체계','시 도 당위원회','도,시','시,군','국가','인민','시 군 당위원회', '초급 당위원회', '당 세포조직','세포조직', '집행위원회','비서처', ' 정 ', ' 군 ',' 도 ',' 시 ',' 성 ',\n",
        "                      '정권기관','입법','집행','인사','직책','보임','겸직' ,'행정','지위,''군사노선', '토의','전반','군대','정치위원','인민군','총정치국','집행부서','외곽단체','사회주의애국청년동맹','조선직업총동맹',\n",
        "                      '조선농업근로자동맹','조선사회주의여성동맹','사상교양','전위대','영도','국무위원회','최고주권기관', '최고인민회의', '내각','중앙검찰소','중앙재판소','사법기관','국무위원장',\n",
        "                      '국무위원회','정령', '총리','부총리', '위원장','임명','해임','최고영도자','총사령관','특사권','국가방위위원회','국방위원회','추대','재추대','입법권','최고주권기관',\n",
        "                      '정기회의', \"임시회의\",'상임위원회','대의원','법령','대내외' ,'제의','제1부위원장', '위원','선출','위원장', '부위원장', '서기장',\"직위자\",'국가예산','심의','승인','조약','비준',\n",
        "                      '폐기권','거수가결','예산위원회', \"법제위원회\", '외교위원회','부문위원회','정책안','법안','휴회','상임위원회', '보충안','심의', '국회','국제의회기구','신임장',\"소환장\",'상무회의',\n",
        "                      '국가관리','국방','집행기관','관리기관','정무원','국가주석','검찰기관','검찰','검찰소','특별검찰소','하급검찰소','상급검찰소','중앙검찰소장','인민재판소','특별재판소','인민참심원',\n",
        "                      '배심원','최고재판기관','중핵','핵무장력 핵시험','핵위협','비핵화','핵무기','핵전쟁','핵무력','열핵무기','핵강국','핵반격','핵억제력','핵타격','핵탄두','조국통일','통일적','평화통일','통일방운',\n",
        "                      '남북통일','자주통일','통일운동','통일대회','통일대진군','남조선','남조선것들','리념','리상','경제부','농업부','최고회의','백두산','해외동포','애국헌신','력사적','력사','전투', '돌파전','지구관측위성','광명성-4','정지위성','운반로켓용','지상분출시험',\n",
        "                      '우주정복','과학연구','노농적위군','2·8비날론연합기업소','전민총돌격전','박근혜','아시아태평양지배전략','제국주의' ,'반동세력','북과 남','국방분야','과학교육','과학기술성과',\n",
        "                      '수소폭탄','수소탄','대륙간 탄도로켓','국방력','핵탄두','7·4공동성명','조국통일3대원칙','6.15공동선언','6.15공동','10.4선언',\"6·15공동선언\",'１０.４선언발표','１０.４선언발표','１０.４','６.１５공동선언', '10·4선언','통일헌장','통일대강','전당초급당위원장대회','초급당조직','아시아태평양지배전략','대아시아지배전략','반공화국전쟁책동',\n",
        "                      '남조선당국','북남당국','남조선호전광','청년동맹','정전협정','평화협정','어머니당','세계평화','로씨야','강성대국건설','령도','영도','어버이','김정은','어버이수령','어버이장군님','직맹', '농근맹', '여맹 조직','선린우호','친선협조관계','친선협조','통일대회합',\n",
        "                      '사대매국','동족대결','군인','사대매국책동','세계평화','평화번영','전성기','중국','미국','외세','미제침략군','백두산영웅청년발전소','청천강계단식발전소','과학기술전당','미래과학자거리','장천남새전문협동농장','사상관철전',\n",
        "                      '당정책옹위전','경제강국건설','협동농장','로농적위군','인공지구위성','반공화국제재','북침전쟁소동','위성과학자주책지구', '김책공업종합대학','연풍과학자휴양소','오중흡7연대 칭호쟁취운동', '근위부대운동','정치사상사업','정치사상강국','애국사업',\n",
        "                      '애국헌신','동포','동지','김일성-김정일주의자','김일성','김정일','조국통일','장병','국방력','군대',' 군 ','조국해방전쟁승리기념관', '은하과학자거리', '문수물놀이장', '마식령스키장',\"전략전술적방침\",\n",
        "                      '６.１２조미공동성명','조미수뇌상봉', '조미','판문점선언','９월평양공동선언','석탄','문명개화기','겨례','남녘','대조선','농기계','알곡고지','농장','과학기술중시기풍','현대과학기술','과학기술','전민과학기술인재화','문화예술','명작','공동선언','풍력','수력','지열','태양열','에네르기','자연에네르기',\n",
        "                      '현대적무장장비','현대과학기술','과학기술위성','전쟁도발책동','사회주의강성국가','천하제일강국','사회주의강국','경제강국','경제건설','실용위성','우주','민족경제','과학화','강성국가','군력','혁명적령군체계','인민군대','조선인민내무군','내무군', '군기','군풍','최정예혁명강군','백두산혁명강군','백두산훈련','전투동원태세','군인','문화후생시설', '공원','유원지',\n",
        "                      '근로단체조직','일꾼','근로인민대중','정치사상','경제관리방법','동족대결정책','애국주의','애국적열의','현신']\n",
        "# 重複削除\n",
        "PoliticalTerms = list(set(PoliticalTerms))\n",
        "print(PoliticalTerms)\n",
        "# 長い単語から処理\n",
        "PoliticalTerms.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtfpiCRMjyEe"
      },
      "source": [
        "## ストップワード辞書\n",
        "以下のサイトで提供されているストップワード辞書を、現在のデータの状態に合わせて修正した。\n",
        "このとき、1文字の単語は誤分類される可能性が高いため、2文字以上のストップワードのみを使用することにする。\n",
        "\n",
        "参考サイト: https://gist.github.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a#file-stopwords-ko-txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zI13PUBJjx4Z"
      },
      "outputs": [],
      "source": [
        "stopwords = []\n",
        "with open('/Users/kimsuyeon/Desktop/Project2_TM/stopwords-ko.txt', 'r') as f:\n",
        "    list_file = f.readlines()\n",
        "\n",
        "for word in list_file:\n",
        "    stopword = word.split('\\n')[0]\n",
        "    if len(stopword) > 1:\n",
        "        stopwords.append(stopword)\n",
        "\n",
        "# 長い単語から処理\n",
        "stopwords.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-19Qw9Skb6N"
      },
      "source": [
        "## 同義語辞書\n",
        "特定の機関を表す用語が複数存在することが確認された。\n",
        "北朝鮮の機関について詳細には分からないため、可能な範囲で同義語辞書を定義する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1t_Uf7mXkgCf"
      },
      "outputs": [],
      "source": [
        "synonym_dict = {\n",
        "    '농촌': ['농촌발전전략','농촌건설강령','농촌','농촌문제해결','농촌건설','농촌마을','농촌발전','농촌생활환경','농촌생활',\"농촌문제\",'농촌경리'],\n",
        "    '농업': ['농업근로자','농업부','농업생산','농업','농업지도일군','밀농사경험','밀농사','농업발전','농업지도','농기계','알곡고지','농장','살림집','협동농장'],\n",
        "    '혁명':['조선혁명','혁명'],\n",
        "    '사회주의':['사회주의',\"공화국\"],\n",
        "    '조선민주주의인민공화국':['조국','조선민주주의인민공화국','국가','나라','조선','당국','조선반도','대조선','우리나라'],\n",
        "    '괴뢰': ['괴뢰패당','괴뢰군부','괴뢰군무력','괴뢰정권','남조선것들','반통일사대매국세력','남조선호전광'],\n",
        "    '대한민국': ['대한민국','남조선','남녘','남조선당국'],\n",
        "    '김정은': ['최고지도자','김정은','총비서','조선로동당 위원장동지'],\n",
        "    '북남': ['북남관계','북남','북과 남','북남당국'],\n",
        "    '민주주의': ['반공화국','자유민주주의','민주주의'],\n",
        "    '미국': ['대미','대미대적','미국','유엔군사령부','외세','미제침략군','６.１２조미공동성명','조미수뇌상봉', '조미','미군'],\n",
        "    '우리': ['우리식', '우리 식','우리당', '우리 당','우리민족','우리 민족','우리힘','우리 힘',\"우리나라\"],\n",
        "    '조선로동당': ['조선로동당', '로동당','당'],\n",
        "    '당중앙위원회': ['당 중앙위원회', '당중앙위원회','중앙위원회'],\n",
        "    '당중앙검사위원회':['당 중앙검사위원회','당중앙검사위원회','중앙검사위원회'],\n",
        "    '당중앙군사위원회':['당 중앙군사위원회','당중앙군사위원회','중앙군사위원회'],\n",
        "    '당중앙위원장제' : ['당 중앙위원장제','당중앙위원장제','중앙위원장제'],\n",
        "    '당대표자회':['당대표자회',\"당 대표자회\",'당 대회','당대회','대회'],\n",
        "    '경제' : ['경제관리방법','인민경제발전','민족경제','경제건설','경제부','경제발전','경제','인민경제','경제강국건설','건설경제력', '경제적', '경제제재', '자립경제','국가경제', '경제발전','경제사업','사회경제','경제구조','지방경제','경제활성화'],\n",
        "    '핵' : ['핵','핵위기사태','중핵','핵무장력 핵시험','핵위협','비핵화','핵무기','핵전쟁','핵무력','핵탄두','열핵무기','핵강국','수소폭탄','대륙간 탄도로켓','수소탄','핵반격','핵억제력','핵타격','핵탄두'],\n",
        "    '통일':['통일','공동선언','조국통일로선','판문점선언','９월평양공동선언','조국통일','통일적','세계평화','평화번영','６.１５','１０.４선언발표','１０.４선언발표','１０.４','６.１５공동선언','6.15공동선언','6.15선언','10.4선언','평화통일','통일방운','평화협정','남북통일','자주통일','통일운동','통일대회','통일대진군','7·4공동성명','조국통일3대원칙', '6·15공동선언', '10·4선언','통일헌장','통일대강'],\n",
        "    '군수':['군수','군수공업'],\n",
        "    '국방':['국방','국방공업','현대적무장장비','병기창','국방력','국방력','군대',' 군 ','인민군','인민군대','군사','장병','전략전술적방침','전략전술','전쟁도발책동','군력','혁명적령군체계','인민군대','조선인민내무군','내무군', '군기','군풍','최정예혁명강군','백두산혁명강군','백두산훈련','전투동원태세','군인'],\n",
        "    '과학':['과학','과학화','과학자','과학자','과학기술','과학교육','과학기술성과','과학연구','과학기술중시기풍','과학기술','전민과학기술인재화','현대과학기술'],\n",
        "    '우주':['실용위성','우주','지구관측위성','위성','광명성-4','정지위성','운반로켓용','지상분출시험','우주정복','과학기술위성','인공지구위성'],\n",
        "    '력사':['력사적','력사'],\n",
        "    '제국주의':['아시아태평양지배전략','반동세력','대아시아지배전략','반공화국전쟁책동','북침전쟁소동','반공화국제재','동족대결','사대매국','사대매국책동'],\n",
        "    '러시아':['로씨야','러시아'],\n",
        "    '김정일':['어버이','김정일','어버이수령','어버이장군님'],\n",
        "    '친선':['친선협조관계','친선협조','친선'],\n",
        "    '애국':['애국사업','애국헌신','애국열','애국미','애국','애국자','애국주의','애국적열의','현신'],\n",
        "    '동지':['동포','동지','겨례'],\n",
        "    '영도':['령도','영도'],\n",
        "    '강국':['사회주의강성국가','천하제일강국','사회주의강국','경제강국','강성국가'],\n",
        "    '정치사상':['정치사상사업','정치사상강국','정치사상'],\n",
        "    '문명':['문명개화기','문화예술','명작','극장','야외극장','출판','문화후생시설', '공원','유원지'],\n",
        "    '에네르기':['풍력','수력','지열','태양열','에네르기','자연에네르기'],\n",
        "    '수산업':['원양어','양어','바다','수산','양식'],\n",
        "    '일꾼':['근로단체조직','일꾼','근로인민대중','로농적위군']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAgV8RuKlBTN"
      },
      "source": [
        "## 北朝鮮用語辞書の活用\n",
        "1) 1文字の単語は誤分類率を高めるため、2文字以上の単語のみを使用する。\n",
        "\n",
        "2) 動詞も含まれているため、「-다」で終わる単語は削除する。\n",
        "\n",
        "\n",
        "参考サイト:https://nkinfo.unikorea.go.kr/nkp/word/nkword.do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "G03KIeLQlUCJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "NorthKoreaTerms = pd.read_csv(\"/Users/kimsuyeon/Desktop/Project2_TM/북한용어사전.csv\")\n",
        "NorthKoreaTerms = [word for word in NorthKoreaTerms[38:]['용어'] if ' ' not in word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0O-wjd5JlErZ"
      },
      "outputs": [],
      "source": [
        "#2)動詞も含まれているため、「-다」で終わる単語は削除する。\n",
        "#1文字の単語と「대한」という単語は使用しない。-> 「대한」は、辞書で定義した「대한민국」と混同する可能性があるため\n",
        "NKTerms = []\n",
        "for word in NorthKoreaTerms:\n",
        "    if ((word[-1] != '다') | (word != '대한')) & (len(word) != 1):\n",
        "        NKTerms.append(word)\n",
        "\n",
        "# 長い単語から処理\n",
        "NKTerms.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSrPB41ZkChq"
      },
      "source": [
        "# 2️⃣必要な式の定義\n",
        "\n",
        "1) 年度別のテキストデータを読み込む式\n",
        "\n",
        "2) 辞書を基準にストップワード（stopwords）を除去する式\n",
        "\n",
        "3) 辞書を基準に名詞を抽出する式\n",
        "\n",
        "4) 同義語をマッピングする式\n",
        "\n",
        "5) text cleaningの式\n",
        ": 1文字の単語やスペースをすべて' 'に置換する\n",
        "\n",
        "6) 結果をExcelに出力する式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "AYc8LueKj6e4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# 1)テキストデータを読み込む式\n",
        "def read_data(sheet_name,header):\n",
        "    return pd.read_excel(\"/Users/kimsuyeon/Desktop/Project2_TM/신년사_통합_0812.xlsx\",sheet_name=sheet_name,header=header)\n",
        "\n",
        "# 2)ストップワード（stopwords）を除去する式\n",
        "def remove_stopwords(text,stopwords_list):\n",
        "    remove_stopwords_text = text\n",
        "    for noun in stopwords_list:\n",
        "        if noun in remove_stopwords_text:\n",
        "                remove_stopwords_text = remove_stopwords_text.replace(noun, \"  \")\n",
        "    return remove_stopwords_text\n",
        "\n",
        "\n",
        "# 3)辞書を基準に名詞を抽出する式\n",
        "from collections import Counter\n",
        "def extract_nouns(text, noun_list):\n",
        "    extracted_nouns = []\n",
        "    remaining_text = text\n",
        "    for noun in noun_list:\n",
        "        if noun in remaining_text:\n",
        "            count = remaining_text.count(noun)\n",
        "            extracted_nouns.append((noun,count))\n",
        "            remaining_text = remaining_text.replace(noun, \" \")\n",
        "    return extracted_nouns, remaining_text\n",
        "\n",
        "#4)同義語をマッピングする式\n",
        "#同義語処理\n",
        "def synonym(word, synonym_dict):\n",
        "    for key, synonyms in synonym_dict.items():\n",
        "        if word in synonyms:\n",
        "            return key\n",
        "    return word\n",
        "\n",
        "def synonyms_freq(data, synonym_dict):\n",
        "    \"\"\" \n",
        "    単語と頻度データを同義語辞書に基づいて統合する関数\n",
        "    Parameters:\n",
        "        data (dict または list): 単語と頻度データを含むdictまたは(単語, 頻度)のタプルリスト\n",
        "        synonym_dict (dict): 同義語辞書で、keyは代表単語、valueは同義語リスト\n",
        "    Returns:\n",
        "        dict: 統合された単語と頻度を含む辞書\n",
        "\"\"\"\n",
        "    # dataがリストの場合、(単語, 頻度)の形式で dict に変換し\n",
        "    if isinstance(data, list):\n",
        "        word_frequencies = {}\n",
        "        for sublist in data:\n",
        "            for word, freq in sublist:\n",
        "                word_frequencies[word] = word_frequencies.get(word, 0) + freq\n",
        "    # dictの場合はそのまま使用\n",
        "    elif isinstance(data, Counter):\n",
        "        word_frequencies = data.copy()\n",
        "\n",
        "    merged_frequencies = {}\n",
        "    # 同義語辞書に基づいて頻度を統合\n",
        "    for main_word, synonym_list in synonym_dict.items():\n",
        "        total_count = word_frequencies.get(main_word, 0)\n",
        "\n",
        "        for synonym in synonym_list:\n",
        "            total_count += word_frequencies.get(synonym, 0)\n",
        "            # 同義語の頻度を合算した後、同義語は削除\n",
        "            if synonym in word_frequencies:\n",
        "                del word_frequencies[synonym]\n",
        "\n",
        "        if total_count > 0:\n",
        "            merged_frequencies[main_word] = total_count\n",
        "\n",
        "    # 同義語として処理されなかった単語も追加\n",
        "    for word, count in word_frequencies.items():\n",
        "        if word not in merged_frequencies:\n",
        "            merged_frequencies[word] = count\n",
        "\n",
        "    return merged_frequencies\n",
        "\n",
        "#5)text cleaning\n",
        "def clean_text(data):\n",
        "    remaining_text =[]\n",
        "    for sentence in data:\n",
        "        sentences = ''\n",
        "        for char in sentence.split(' '):\n",
        "            if len(char)>1:\n",
        "                sentences += char + ' '\n",
        "        remaining_text.append(sentences)\n",
        "    return remaining_text\n",
        "\n",
        "#6)結果をExcelに出力する式\n",
        "#単語-頻度の結果を保存するデータセット\n",
        "doc_word_freq = {}\n",
        "file_path = '/Users/kimsuyeon/Desktop/doc_word_freq.csv'\n",
        "\n",
        "def export_to_csv(doc_word_freq, file_path):\n",
        "    # すべての単語を収集\n",
        "    all_words = set()\n",
        "    for freq_dict in doc_word_freq.values():\n",
        "        all_words.update(freq_dict.keys())\n",
        "\n",
        "    all_words = sorted(all_words)\n",
        "\n",
        "    # DataFrameに変換\n",
        "    data = []\n",
        "    for doc_name, freq_dict in doc_word_freq.items():\n",
        "        row = [freq_dict.get(word, 0) for word in all_words]\n",
        "        data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(data, index=doc_word_freq.keys(), columns=all_words)\n",
        "    df.to_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8EdYPA2brFc"
      },
      "source": [
        "# 3️⃣単語のトークナイズと単語頻度に基づく可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y1smDFVmZp5"
      },
      "source": [
        "## 📌2024年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3PqN6-cImgcx"
      },
      "outputs": [],
      "source": [
        "df_23end = read_data(0,1) #sheet1:2023.12.31\n",
        "all = df_23end['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "\n",
        "all_23end = []\n",
        "for text in all:\n",
        "    all_23end.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bo7h5Hqlm4gE"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 名前抽出\n",
        "extracted_name = []\n",
        "for sen in all_23end:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                split_char = char.split('동지')\n",
        "                if len(split_char) > 1:\n",
        "                    extracted_name.append(split_char[-2][-3:])\n",
        "#　名前頻度\n",
        "name,texts_without_name  = [],[]\n",
        "# 既存のテキストから、前に抽出された名前を削除\n",
        "for text in all_23end:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) # 既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存\n",
        "    texts_without_name.append(remaining_text) # extracted_nameに含まれる名前を削除したテキストを作成\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_23end = Counter(word_frequencies_name) # 最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgPMgwETqb1j"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "G6bgK99Iqcaz"
      },
      "outputs": [],
      "source": [
        "# 名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "# 同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_23end = Counter(word_frequencies_PoliticalTerms) # 最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnPZgKLg6CEh"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PcJ4QlxzscRA"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqYnb6D-7MJr"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPga5OcAsnkP"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "# soynlpで抽出された名詞のうち、2文字以上のみ保存 \n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_23end =Counter(extracted_soynlp_words)\n",
        "\n",
        "# soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNoY_aRm7xSe"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WtaJVGXaszu9"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "# 同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_23end = Counter(word_frequencies_NKterms) # 最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn-c0XWd7z7Z"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsKMwjFUBgAe"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean]\n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_23end = Counter(mecab_nouns)\n",
        "mecab_23end\n",
        "\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X0ppxol715x"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRGJUhlutF2r"
      },
      "outputs": [],
      "source": [
        "word_all_23end = mecab_23end+soynlp_23end+name_23end+NKterms_23end+PoliticalTerms_23end\n",
        "combined_all_23end = synonyms_freq(word_all_23end, synonym_dict)\n",
        "\n",
        "combined_all_23end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyV_xctqtYXI"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RhpwXfB6tzV1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-C9VkytbZs"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J48g2mr5tRPW"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joj0nTjntjVI"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XAVIibtnNu"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_oFEvcPtupl"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsYmXS4ttan"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GmzcN8FS5v"
      },
      "source": [
        "## 📌2023年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GrRqMBf4Fchn"
      },
      "outputs": [],
      "source": [
        "df_23be = read_data(1,2) #sheet2: 2023.01.01\n",
        "\n",
        "all = df_23be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'を基準に文を分割\n",
        "all = all.split('\\n\\n\\u3000\\u3000')\n",
        "#　特殊記号('1. ','2. ','3. ', '4. ' ,'\\n'など)を削除\n",
        "for idx,sent in enumerate(all):\n",
        "    if '1. ' in sent or '2. 'in sent or '3. ' in sent or '4. ' in sent or '5. 'in sent or '6. 'in sent:\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_23be = []\n",
        "for text in all:\n",
        "    all_23be.append(re.sub(r'[\\n\\u3000]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uZxy-HDlF1kR"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 名前\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_23be:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#　名前頻度\n",
        "name,texts_without_name  = [],[]\n",
        "#　既存のテキストから、前に抽出された名前を削除する。\n",
        "for text in all_23be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #　既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存\n",
        "    texts_without_name.append(remaining_text) #extracted_nameに含まれる名前を削除したテキストを作成する\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_23be = Counter(word_frequencies_name) #　最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDyecVlzF1kS"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "X5CzmYFUF1kS"
      },
      "outputs": [],
      "source": [
        "#　名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#　同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_23be = Counter(word_frequencies_PoliticalTerms)#　最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxDMgyAoF1kT"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "TdfUMwnjF1kT"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU8wMcd4F1kT"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCX2ceW4F1kU"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#　soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_23be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#　soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYE8vBDyF1kV"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vZmvBbz2F1kV"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#　同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_23be = Counter(word_frequencies_NKterms) # 最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiO8RGoyF1kW"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7fho_5MF1kX"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #　同義語の処理\n",
        "\n",
        "#　単語頻度を計算\n",
        "mecab_23be = Counter(mecab_nouns)\n",
        "mecab_23be\n",
        "\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxm6mWKbF1kX"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzhZTVm3F1kX"
      },
      "outputs": [],
      "source": [
        "word_all_23be = mecab_23be+soynlp_23be+name_23be+NKterms_23be+PoliticalTerms_23be\n",
        "combined_all_23be = synonyms_freq(word_all_23be, synonym_dict)\n",
        "\n",
        "combined_all_23be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXad4mRwF1kY"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "s2EFRh0CF1kY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_2cx1hrF1kZ"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTKIKz9iF1kZ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk_ekaOFF1kZ"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHmXCIrXF1ka"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3ruAaSJF1ka"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP9h07vSF1ka"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z58SFXzaHble"
      },
      "source": [
        "## 📌2022年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5hjYwyr8Hblf"
      },
      "outputs": [],
      "source": [
        "df_22be = read_data(2,3) #sheet3: 2022.01.01\n",
        "\n",
        "all = df_22be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号('1. ','2. ','3. ', '4. ' ,'\\n'など)を削除\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_22be = []\n",
        "for text in all:\n",
        "    all_22be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "zL5SclwvHblg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#　名前\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_22be:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#　名前の頻度\n",
        "name,texts_without_name  = [],[]\n",
        "#既存のテキストから、前に抽出された名前を削除する\n",
        "for text in all_22be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存する。\n",
        "    texts_without_name.append(remaining_text) #extracted_nameに含まれる名前を削除したテキストを作成する。\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_22be = Counter(word_frequencies_name) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YD30MSWHblg"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "yBLCl71DHblg"
      },
      "outputs": [],
      "source": [
        "#　名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#　同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_22be = Counter(word_frequencies_PoliticalTerms) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gABu1rCRHblg"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "WI6cHASQHblh"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6aNdYZ-Hblh"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2XnDyMxHblh"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#　soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_22be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpで抽出された名詞を texts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVVuUpKCHbli"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5HKraiTFHbli"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_22be = Counter(word_frequencies_NKterms) \n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoEKyabjHblj"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb9y0CWTHblj"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #　同義語の処理\n",
        "\n",
        "#　単語頻度を計算\n",
        "mecab_22be = Counter(mecab_nouns)\n",
        "mecab_22be\n",
        "\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMgCX_yZHblj"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a9BxjhKHblj"
      },
      "outputs": [],
      "source": [
        "word_all_22be = mecab_22be+soynlp_22be+name_22be+NKterms_22be+PoliticalTerms_22be\n",
        "combined_all_22be = synonyms_freq(word_all_22be, synonym_dict)\n",
        "\n",
        "combined_all_22be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juGOGPVOHblk"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "EegAVFIMHblk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlsBwRvbHblk"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqBowy4CHbll"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrBcH76DHbll"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEIzlCrbHbll"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHj5h6G5Hbll"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABA31ixKHblm"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdYYiWT_JBkr"
      },
      "source": [
        "## 📌2021年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "iIgLcdhcJcZK"
      },
      "outputs": [],
      "source": [
        "df_21be = read_data(3,3) #sheet4: 2021.01.01\n",
        "\n",
        "all = df_21be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_21be = []\n",
        "for text in all:\n",
        "    all_21be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "F681wS4vJcZZ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#名前\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_21be:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#名前の頻度\n",
        "name,texts_without_name  = [],[]\n",
        "#既存のテキストから、前に抽出された名前を削除\n",
        "for text in all_21be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #　既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存\n",
        "    texts_without_name.append(remaining_text) #　extracted_nameに含まれる名前を削除したテキストを作成\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_21be = Counter(word_frequencies_name) #　最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gqO7_ytJcZa"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "svJ5F1BTJcZa"
      },
      "outputs": [],
      "source": [
        "#　名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#　同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_21be = Counter(word_frequencies_PoliticalTerms)#　最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFs8bRrRJcZa"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "qdgLcW9YJcZa"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF7ZEOIKJcZb"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoqqkKlAJcZb"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#　soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_21be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#　soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwXXlCWpJcZc"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "cEDNoExMJcZc"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#　同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_21be = Counter(word_frequencies_NKterms) \n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_LDqiGJcZc"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FgxIYdAJcZc"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean]\n",
        "\n",
        "#　単語頻度を計算\n",
        "mecab_21be = Counter(mecab_nouns)\n",
        "mecab_21be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzeN_4T_JcZd"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "i0cLUmRhJcZd"
      },
      "outputs": [],
      "source": [
        "word_all_21be = mecab_21be+soynlp_21be+name_21be+NKterms_21be+PoliticalTerms_21be\n",
        "combined_all_21be = synonyms_freq(word_all_21be, synonym_dict)\n",
        "\n",
        "combined_all_21be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBAdl25EJcZd"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "VTP-xwcpJcZd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbueJrQJcZe"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaQYkD7dJcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqFSX-IVJcZe"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7hqLrB1JcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21cKhCxVJcZe"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_wRZgnhJcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5-V4lNjJEDL"
      },
      "source": [
        "## 📌2020年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "sG9j2KFKKNjY"
      },
      "outputs": [],
      "source": [
        "df_20be = read_data(4,2)\n",
        "\n",
        "all = df_20be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_20be = []\n",
        "for text in all:\n",
        "    all_20be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "HzJVaOssKNjm"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 名前\n",
        "extracted_name = []\n",
        "for sen in all_20be:\n",
        "    if '동지' in sen :\n",
        "        word = sen.split(' ')\n",
        "        for idx,char in enumerate(word):\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')\n",
        "                if name[0] == '':\n",
        "                    extracted_name.append(word[idx-1])\n",
        "                elif name[0] != '위원장':\n",
        "                    extracted_name.append(name[0])\n",
        "\n",
        "#　名前の頻度\n",
        "name,texts_without_name  = [],[]\n",
        "#　既存のテキストから、前に抽出された名前を削除\n",
        "for text in all_20be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #　既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存\n",
        "    texts_without_name.append(remaining_text) #　extracted_nameに含まれる名前を削除したテキストを作成する。\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_20be = Counter(word_frequencies_name) #　最後にすべてのデータを結合する際、+ を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAro0AysKNjm"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "c2R1Yt80KNjm"
      },
      "outputs": [],
      "source": [
        "#　名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#　同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_20be = Counter(word_frequencies_PoliticalTerms)#　最後にすべてのデータを結合する際、+ を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u01v1NMgKNjm"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "0lCLrH-pKNjm"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiVA_0NxKNjm"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z1UUzCgKNjn"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#　soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_20be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#　soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ArVYL36KNjn"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Z36_EuslKNjn"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#　同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_20be = Counter(word_frequencies_NKterms) #　最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTay7KfUKNjo"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI5Fb0utKNjo"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #　同義語の処理\n",
        "\n",
        "#　単語頻度を計算\n",
        "mecab_20be = Counter(mecab_nouns)\n",
        "mecab_20be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rwmcdY-KNjo"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FqKwNW9KNjo"
      },
      "outputs": [],
      "source": [
        "word_all_20be = mecab_20be+soynlp_20be+name_20be+NKterms_20be+PoliticalTerms_20be\n",
        "combined_all_20be = synonyms_freq(word_all_20be, synonym_dict)\n",
        "\n",
        "combined_all_20be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8DwGC6KNjo"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Lqoe5NGvKNjp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPhZxxREKNjp"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxoFUfMqKNjp"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3k8Ul5YKNjp"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPXMjJUhKNjp"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtdhFgA9KNjq"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM8Y15vHKNjq"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22tveb6HJGWW"
      },
      "source": [
        "## 📌2019年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "lYqrSQ9fM80c"
      },
      "outputs": [],
      "source": [
        "df_19be = read_data(5,3)\n",
        "\n",
        "all = df_19be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_19be = []\n",
        "for text in all:\n",
        "    all_19be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_19be = all_19be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "-gazII3lM80o"
      },
      "outputs": [],
      "source": [
        "# 全文に名前が含まれていない。\n",
        "# from collections import Counter\n",
        "\n",
        "# #名前\n",
        "# extracted_name = []\n",
        "# for sen in all_19be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #名前の頻度\n",
        "# name,texts_without_name  = [],[]\n",
        "# #既存のテキストから、前に抽出された名前を削除する。\n",
        "# for text in all_19be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存する。\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameに含まれる名前を削除したテキストを作成する。\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_19be = Counter(word_frequencies_name) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVF1vFYjM80p"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "PNDzHn_2M80p"
      },
      "outputs": [],
      "source": [
        "#　名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_19be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#　同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_19be = Counter(word_frequencies_PoliticalTerms)#最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQZlmH_M80p"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "kI5ZJARDM80p"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViwprXmxM80q"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw7h50CjM80q"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_19be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ5LDHDsM80q"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "LFD2YkYxM80r"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#　同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_19be = Counter(word_frequencies_NKterms) #最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jh0NYkJM80r"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAqyjCfUM80r"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] \n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_19be = Counter(mecab_nouns)\n",
        "mecab_19be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmBSlw9bM80r"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "8j8nUCOmM80s"
      },
      "outputs": [],
      "source": [
        "word_all_19be = mecab_19be+soynlp_19be+NKterms_19be+PoliticalTerms_19be\n",
        "combined_all_19be = synonyms_freq(word_all_19be, synonym_dict)\n",
        "\n",
        "combined_all_19be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-eVngsxM80s"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ZhKqiNleM80s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9II_w4aM80t"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubnDMMI4M80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59InN9_WM80t"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQdAxaWdM80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKb9CQgM80t"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4jlFsoYM80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkdS5p_JIBI"
      },
      "source": [
        "## 📌2018年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "k9NCsSV0NCXr"
      },
      "outputs": [],
      "source": [
        "df_18be = read_data(6,3)\n",
        "\n",
        "all = df_18be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_18be = []\n",
        "for text in all:\n",
        "    all_18be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_18be = all_18be[1:]\n",
        "all_18be[-1] = all_18be[-1][:60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "XJnkR24nNCXs"
      },
      "outputs": [],
      "source": [
        "#全文に名前が含まれていない。\n",
        "# from collections import Counter\n",
        "\n",
        "# #名前\n",
        "# extracted_name = []\n",
        "# for sen in all_18be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #名前の頻度\n",
        "# name,texts_without_name  = [],[]\n",
        "# #既存のテキストから、前に抽出された名前を削除する。\n",
        "# for text in all_18be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存する。\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameに含まれる名前を削除したテキストを作成する。\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_18be = Counter(word_frequencies_name) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmhAMB7BNCXt"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "VlOMlCHKNCXt"
      },
      "outputs": [],
      "source": [
        "#名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_18be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_18be = Counter(word_frequencies_PoliticalTerms)#最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjhkbeb2NCXt"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "gR8HlipyNCXt"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' ' のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTEeGosNCXu"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQe3JgVlNCXu"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_18be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azMyo3QENCXv"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "ZUbVfzm1NCXv"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_18be = Counter(word_frequencies_NKterms) #最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0MYxjeNCXw"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCWj2CE7NCXw"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean]\n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_18be = Counter(mecab_nouns)\n",
        "mecab_18be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDPtrpBcNCXw"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "zbdOj7gZNCXw"
      },
      "outputs": [],
      "source": [
        "word_all_18be = mecab_18be+soynlp_18be+NKterms_18be+PoliticalTerms_18be\n",
        "combined_all_18be = synonyms_freq(word_all_18be, synonym_dict)\n",
        "\n",
        "combined_all_18be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTrBeE1TNCXx"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "4iUJ8A6zNCXx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkR0ImEgNCXx"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3q_RAtHNCXx"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCBfHBwrNCXy"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mUm6kv2NCXy"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epWQ5PLiNCXy"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC3QYoLANCXy"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PPcav8cJJul"
      },
      "source": [
        "## 📌2017年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "XFd854PRNOLN"
      },
      "outputs": [],
      "source": [
        "df_17be = read_data(7,3)\n",
        "all = df_17be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_17be = []\n",
        "for text in all:\n",
        "    all_17be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "5nG4baiqNOLN"
      },
      "outputs": [],
      "source": [
        "#全文に名前が含まれていない。\n",
        "# from collections import Counter\n",
        "\n",
        "# #名前\n",
        "# extracted_name = []\n",
        "# for sen in all_17be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #名前の頻度\n",
        "# name,texts_without_name  = [],[]\n",
        "# #既存のテキストから、前に抽出された名前を削除する。\n",
        "# for text in all_17be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存する。\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameに含まれる名前を削除したテキストを作成する。\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_17be = Counter(word_frequencies_name) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZAPxsK9NOLN"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "f1a5MvJ4NOLO"
      },
      "outputs": [],
      "source": [
        "#名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_17be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_17be = Counter(word_frequencies_PoliticalTerms)#最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kec6OZCWNOLO"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "vazu2cpKNOLO"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOwTLhnNOLO"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRVa4rmxNOLO"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_17be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v82QA13NOLO"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "q7BxfvX6NOLP"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_17be = Counter(word_frequencies_NKterms) #最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N_aSo-fNOLP"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF0I7brMNOLP"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] \n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_17be = Counter(mecab_nouns)\n",
        "mecab_17be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGaqz7gHNOLP"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "pkusM_6QNOLP"
      },
      "outputs": [],
      "source": [
        "word_all_17be = mecab_17be+soynlp_17be+NKterms_17be+PoliticalTerms_17be\n",
        "combined_all_17be = synonyms_freq(word_all_17be, synonym_dict)\n",
        "\n",
        "combined_all_17be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PFdmCS5NOLP"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "-4Ot7VMkNOLQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6q0I-c-NOLQ"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS0qEncVNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAyg6tMBNOLQ"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISFIkHeVNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhB2ul2NOLQ"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oer1DOyQNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6B6dTJLwB"
      },
      "source": [
        "## 📌2016年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Miv3XqMYNMkv"
      },
      "outputs": [],
      "source": [
        "df_16be = read_data(8,3)\n",
        "\n",
        "all = df_16be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_16be = []\n",
        "for text in all:\n",
        "    all_16be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "Ju5zdLrINMk8"
      },
      "outputs": [],
      "source": [
        "# 全文に名前が含まれていない。\n",
        "# from collections import Counter\n",
        "\n",
        "# #名前\n",
        "# extracted_name = []\n",
        "# for sen in all_16be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #名前の頻度\n",
        "# name,texts_without_name  = [],[]\n",
        "# #既存のテキストから、前に抽出された名前を削除する。\n",
        "# for text in all_16be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存する。\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameに含まれる名前を削除したテキストを作成する。\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_16be = Counter(word_frequencies_name) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjzO2h84NMk8"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "rxa-cJ91NMk8"
      },
      "outputs": [],
      "source": [
        "#名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_16be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_16be = Counter(word_frequencies_PoliticalTerms)#最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMtQ27NMNMk8"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "fjfjznAMNMk9"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-AUBZEGNMk9"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9lg1_2WNMk9"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_16be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5k-7HWNMk9"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "n23d2TiCNMk9"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_16be = Counter(word_frequencies_NKterms) #最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ9ls1BQNMk-"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc4AE65ONMk-"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #同義語の処理\n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_16be = Counter(mecab_nouns)\n",
        "mecab_16be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQk4lTDVNMk-"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "ajU76TIZNMk-"
      },
      "outputs": [],
      "source": [
        "word_all_16be = mecab_16be+soynlp_16be+NKterms_16be+PoliticalTerms_16be\n",
        "combined_all_16be = synonyms_freq(word_all_16be, synonym_dict)\n",
        "\n",
        "combined_all_16be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-_3IjhNMk_"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Ri1tmJJ4NMk_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wNh9Jv9NMk_"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDIye0EdNMk_"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vve8JzyFNMk_"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPAiVlUHNMk_"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9s4eDZ3NMlA"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9w_j_RQNMlA"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKdXB7B-JM6p"
      },
      "source": [
        "## 📌2015年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "eK-IpVQKNKxy"
      },
      "outputs": [],
      "source": [
        "df_15be = read_data(9,3)\n",
        "\n",
        "all = df_15be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_15be = []\n",
        "for text in all:\n",
        "    all_15be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_15be = all_15be[2:]\n",
        "all_15be = all_15be[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB-Em_9ENKxz"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "jkuDL1hjNKxz"
      },
      "outputs": [],
      "source": [
        "# 名詞抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_15be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_15be = Counter(word_frequencies_PoliticalTerms)#最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8jqhjfBNKxz"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "9xjKgH7oNKxz"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-whWJvNKxz"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF-GDXwbNKx0"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_15be = Counter(extracted_soynlp_words)\n",
        "\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIB9V8ZsNKx0"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "jsQ75MKVNKx0"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_15be = Counter(word_frequencies_NKterms) \n",
        "\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOfatEjRNKx0"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a055JmQdNKx1"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] \n",
        "\n",
        "mecab_15be = Counter(mecab_nouns)\n",
        "mecab_15be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ywZH9AVNKx1"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "_qfTmPQuNKx1"
      },
      "outputs": [],
      "source": [
        "word_all_15be = mecab_15be+soynlp_15be+NKterms_15be+PoliticalTerms_15be\n",
        "combined_all_15be = synonyms_freq(word_all_15be, synonym_dict)\n",
        "\n",
        "combined_all_15be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQSGZKVRNKx2"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "AbipL2AtNKx2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsKmwR7TNKx2"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MlQIVPMNKx2"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bus9GfSGNKx2"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZqJERtLNKx3"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzbL8Rn4NKx3"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCMxcvyJNKx3"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqTAB8JJOcj"
      },
      "source": [
        "## 📌2014年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "AeJ7M0IcNInN"
      },
      "outputs": [],
      "source": [
        "df_14be = read_data(10,2)\n",
        "\n",
        "all = df_14be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_14be = []\n",
        "for text in all:\n",
        "    all_14be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_14be[0] = all_14be[0].split(\".\")[2]\n",
        "all_14be = all_14be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "y4q2fH2TNInY"
      },
      "outputs": [],
      "source": [
        "# 全文に名前が含まれていない。\n",
        "# from collections import Counter\n",
        "\n",
        "# #名前\n",
        "# extracted_name = []\n",
        "# for sen in all_14be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #名前の頻度\n",
        "# name,texts_without_name  = [],[]\n",
        "# #既存のテキストから、前に抽出された名前を削除する。\n",
        "# for text in all_14be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #既存のテキストからextracted_nameで抽出された名前と一致する場合、その名前をnameに保存する。\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameに含まれる名前を削除したテキストを作成する。\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_14be = Counter(word_frequencies_name) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK_hPV4XNInY"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "qb8wiyuGNInZ"
      },
      "outputs": [],
      "source": [
        "#名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_14be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_14be = Counter(word_frequencies_PoliticalTerms)#最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQpSMbANInZ"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "b2JaJECINInZ"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIQlcwAWNInZ"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8OhcwxtNInZ"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#　soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_14be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#　soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQH-oCDhNIna"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "B-31J1MeNIna"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#　同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_14be = Counter(word_frequencies_NKterms) #最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePV0uIMGNIna"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7tY-XlBNIna"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #同義語の処理\n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_14be = Counter(mecab_nouns)\n",
        "mecab_14be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biryHpFuNIna"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVY_O3jYNInb"
      },
      "outputs": [],
      "source": [
        "word_all_14be = mecab_14be+soynlp_14be+NKterms_14be+PoliticalTerms_14be\n",
        "combined_all_14be = synonyms_freq(word_all_14be, synonym_dict)\n",
        "\n",
        "combined_all_14be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fTaB7mNInb"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "zc7Ol5YpNInb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaeq3qi-NInb"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55gsTE_dNInb"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtcaOvm8NInb"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P42ocOVENInc"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we8fwgnDNInc"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvdL5rsvNInc"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvNUjt_cJQA_"
      },
      "source": [
        "## 📌2013年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "kqvPis-1NGO4"
      },
      "outputs": [],
      "source": [
        "df_13be = read_data(11,3)\n",
        "\n",
        "all = df_13be['전문'][0]\n",
        "\n",
        "#　特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#　'\\n\\n' を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#　特殊記号を二次的に削除（'1. '、'2. '、'3. '、'4. '、'\\n'など）\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_13be = []\n",
        "for text in all:\n",
        "    all_13be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jegm7dJNGPI"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "Qv81aYQ-NGPJ"
      },
      "outputs": [],
      "source": [
        "#名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_13be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_13be = Counter(word_frequencies_PoliticalTerms)#最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyM7crpmNGPJ"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "Oj52IF2gNGPJ"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' ' のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPEFpuluNGPJ"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlV7XoVINGPJ"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_13be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMXD5tdkNGPK"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "CIkRH71CNGPK"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_13be = Counter(word_frequencies_NKterms) #最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AkmBHV2NGPK"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh0GWidJNGPL"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] \n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_13be = Counter(mecab_nouns)\n",
        "mecab_13be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPJ22egZNGPL"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdBZ1RhRNGPL"
      },
      "outputs": [],
      "source": [
        "word_all_13be = mecab_13be+soynlp_13be+NKterms_13be+PoliticalTerms_13be\n",
        "combined_all_13be = synonyms_freq(word_all_13be, synonym_dict)\n",
        "\n",
        "combined_all_13be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1m4XORqNGPM"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "QF7JUkqsNGPM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL-L8UONNGPM"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueVMRYicNGPM"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0taFNtbNGPN"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r1pB1cLNGPN"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5gf40VNGPN"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP_f25AkNGPN"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TYJKz4YJRxv"
      },
      "source": [
        "## 📌2012年の新年演説"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "qkN-dmTgNDRU"
      },
      "outputs": [],
      "source": [
        "df_12be = read_data(12,3)\n",
        "\n",
        "all = df_12be['전문'][0]\n",
        "\n",
        "#特殊記号を一次的に削除\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'を基準に文を分割\n",
        "all = all.split('\\n\\n')\n",
        "#特殊記号('1. ','2. ','3. ', '4. ' ,'\\n'など)を削除\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#　正しく削除されなかったものを再度削除\n",
        "all_12be = []\n",
        "for text in all:\n",
        "    all_12be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHqu_erqNDRV"
      },
      "source": [
        "### 権力構造と政治形態に関する独自の用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "cjN2lc7LNDRV"
      },
      "outputs": [],
      "source": [
        "#名詞の抽出\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_12be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#同義語の処理\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_12be = Counter(word_frequencies_PoliticalTerms) #最後にすべてのデータを結合する際、+を使用するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6DZU8SZNDRV"
      },
      "source": [
        "### Stopwords削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "Jymzkh9SNDRV"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'のような空白を削除\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqqaxrXxNDRW"
      },
      "source": [
        "### soynlp名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTLmQf8tNDRW"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpで抽出された名詞のうち、2文字以上のみ保存\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_12be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpで抽出された名詞をtexts_without_stopwordsから削除\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDgX8r0NDRW"
      },
      "source": [
        "### 北朝鮮用語辞書を活用して名詞を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "ee9wuVy9NDRW"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#同義語の処理\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_12be = Counter(word_frequencies_NKterms) #最後にすべてのデータを結合する際、+を使用するため\n",
        "\n",
        "#' 'のような空白を削除\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHVX-NpNDRX"
      },
      "source": [
        "### Mecab名詞抽出器（すでに名詞として分類された単語は除外）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWPBx2O_NDRX"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #同義語の処理\n",
        "\n",
        "#単語頻度を計算\n",
        "mecab_12be = Counter(mecab_nouns)\n",
        "mecab_12be\n",
        "\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgD4NkxPNDRY"
      },
      "source": [
        "### すべて統合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuegRlNLNDRY"
      },
      "outputs": [],
      "source": [
        "word_all_12be = mecab_12be+soynlp_12be+NKterms_12be+PoliticalTerms_12be\n",
        "combined_all_12be = synonyms_freq(word_all_12be, synonym_dict)\n",
        "\n",
        "combined_all_12be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxUYclp0NDRZ"
      },
      "source": [
        "### 単語頻度の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "lBpVGOXKNDRZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnP4xTdHNDRa"
      },
      "source": [
        "#### 上位10単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIsZ7HSZNDRa"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位10単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Guog4SyNDRa"
      },
      "source": [
        "#### 上位20単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKAUIPJHNDRa"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位20単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIa2HW0uNDRb"
      },
      "source": [
        "#### 上位30単語（全文）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSCrXH86NDRb"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"上位30単語\", fontproperties=fontprop)\n",
        "plt.ylabel(\"頻度\", fontproperties=fontprop)\n",
        "plt.xlabel(\"名詞\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8MFxw5SZwyn"
      },
      "source": [
        "## 結果をExcelに出力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_word_freq = {}\n",
        "doc_word_freq['23end'] = combined_all_23end\n",
        "doc_word_freq['23be'] = combined_all_23be\n",
        "doc_word_freq['22be'] = combined_all_22be\n",
        "doc_word_freq['21be'] = combined_all_21be\n",
        "doc_word_freq['20be'] = combined_all_20be\n",
        "doc_word_freq['19be'] = combined_all_19be\n",
        "doc_word_freq['18be'] = combined_all_18be\n",
        "doc_word_freq['17be'] = combined_all_17be\n",
        "doc_word_freq['16be'] = combined_all_16be\n",
        "doc_word_freq['15be'] = combined_all_15be\n",
        "doc_word_freq['14be'] = combined_all_14be\n",
        "doc_word_freq['13be'] = combined_all_13be\n",
        "doc_word_freq['12be'] = combined_all_12be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export_to_csv(doc_word_freq, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4️⃣Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "from striprtf.striprtf import rtf_to_text\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 文書-単語行列で構成されたデータセットを読み込む\n",
        "doc_term = pd.read_csv('/Users/kimsuyeon/Desktop/doc_word_freq.csv',index_col=0)\n",
        "#単語抽出\n",
        "terms = list(doc_term.columns)\n",
        "\n",
        "#ストップワード（stopwords）抽出\n",
        "with open('/Users/kimsuyeon/Desktop/remove_words.rtf', 'r', encoding='utf-8') as f:\n",
        "    rtf_content = f.read()\n",
        "remove_list = rtf_to_text(rtf_content)\n",
        "stopwords = set([word.strip() for word in remove_list.splitlines() if word.strip()])\n",
        "\n",
        "#文書ごとの単語リストを作成\n",
        "documents_words = []\n",
        "for doc in doc_term.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        # 用語を削除 +（単語、頻度）で構成されたテキストデータセット\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(documents_words)\n",
        "#頻度フィルタリング\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) # 最低2回以上出現し、文書の90%を占めない単語を抽出\n",
        "#Bag-of-Words生成\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 最適なトピック数を見つける方法(1)Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perplexity_values = []\n",
        "for i in range(2,8):\n",
        "  lda_model = LdaModel(corpus,num_topics=i,id2word=dictionary,random_state=42,passes=7,alpha=1.0,eta=0.5)\n",
        "  perplexity_values.append(lda_model.log_perplexity(corpus))\n",
        "\n",
        "plt.plot(range(2,8),perplexity_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"perplexity score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 最適なトピック数を見つける方法(2)coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coherence_values = []\n",
        "for i in range(2,8):\n",
        "  lda_model = LdaModel(corpus,num_topics=i,id2word=dictionary,random_state=42,passes=7,alpha=1.0,eta=0.5)\n",
        "  coherence_model_lda = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,topn=10)\n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  coherence_values.append(coherence_lda)\n",
        "\n",
        "plt.plot(range(2,8),coherence_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"coherence score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 最適なトピック数：3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#各文書ごとのトピック分布を保存\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = doc_term.index\n",
        "df.to_excel('doc_topic_distributions.xlsx', index=False)\n",
        "\n",
        "#視覚化\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df #1: 国防, 2: 政治, 3: 社会"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 参考（最適なハイパーパラメータを見つける）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_range = range(2, 8)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = doc_term.index\n",
        "df.to_excel('doc_topic_distributions_best1_(1).xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_best1_(1).html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 時期別"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T1 = doc_term.loc[['12be', '13be', '14be', '15be', '16be']]  #2012-2016\n",
        "T2 = doc_term.loc[['17be', '18be', '19be']]                  #2017-2019\n",
        "covid_period = doc_term.loc[['20be', '21be']]                #2020-2021\n",
        "T3 = doc_term.loc[['22be', '23be','23end']]                  #2022-2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T1 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 最適な組み合わせを見つける"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#文書ごとの単語リスト\n",
        "documents_words = []\n",
        "for doc in T1.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語、頻度）で構成されたtext dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #最低2回以上出現し、文書の90%を占めない単語を抽出\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T1.index\n",
        "df.to_excel('doc_topic_distributions_T1.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T1.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### モデリング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T1.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語, 頻度）で構成された\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #最低2回以上出現し、文書の90%を占めない単語を抽出\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#各文書ごとのトピック分布を保存\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T1.index\n",
        "df.to_excel('doc_topic_distributions_T1.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T1.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T2 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 最適な組み合わせを見つける"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#文書ごとの単語リスト\n",
        "documents_words = []\n",
        "for doc in T2.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語、頻度）で構成されたtext dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #最低2回以上出現し、文書の90%を占めない単語を抽出\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T2.index\n",
        "df.to_excel('doc_topic_distributions_T2.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T2.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### モデリング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T2.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語、頻度）で構成されたtext dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #最低2回以上出現し、文書の90%を占めない単語を抽出\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#各文書ごとのトピック分布を保存\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T2.index\n",
        "df.to_excel('doc_topic_distributions_T2.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T2.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Covid Period (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 最適な組み合わせを見つける"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#文書ごとの単語リスト\n",
        "documents_words = []\n",
        "for doc in covid_period.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語、頻度）で構成されたtext dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) \n",
        "#最低2回以上出現し、文書の90%を占めない単語を抽出（該当期間のテキストデータセットは非常に少ないため、ハイパーパラメータの調整が必要）\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = covid_period.index\n",
        "df.to_excel('doc_topic_distributions_covid_period.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_covid_period.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### モデリング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in covid_period.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語、頻度）で構成されたtext dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.98) \n",
        "#最低2回以上出現し、文書の90%を占めない単語を抽出（該当期間のテキストデータセットは非常に少ないため、ハイパーパラメータの調整が必要）\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9#,\n",
        "    #alpha=1.0,\n",
        "    #eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#各文書ごとのトピック分布を保存\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = covid_period.index\n",
        "df.to_excel('doc_topic_distributions_covid_period.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_covid_period.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T3 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 最適な組み合わせを見つける"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#文書ごとの単語リスト\n",
        "documents_words = []\n",
        "for doc in T3.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語、頻度）で構成されたtext dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #最低2回以上出現し、文書の90%を占めない単語を抽出\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T3.index\n",
        "df.to_excel('doc_topic_distributions_T3.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T3.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### モデリング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T3.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #用語を削除 +（単語、頻度）で構成されたtext dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #最低2回以上出現し、文書の90%を占めない単語を抽出\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#各文書ごとのトピック分布を保存\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T3.index\n",
        "df.to_excel('doc_topic_distributions_T3.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T3.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sSrPB41ZkChq",
        "7y1smDFVmZp5",
        "YyV_xctqtYXI",
        "D-GmzcN8FS5v",
        "Z58SFXzaHble",
        "juGOGPVOHblk",
        "KdYYiWT_JBkr",
        "bBAdl25EJcZd",
        "s5-V4lNjJEDL",
        "0K8DwGC6KNjo",
        "22tveb6HJGWW",
        "E-eVngsxM80s",
        "CSkdS5p_JIBI",
        "yTrBeE1TNCXx",
        "2PPcav8cJJul",
        "7PFdmCS5NOLP",
        "wjf6B6dTJLwB",
        "Zw-_3IjhNMk_",
        "hKdXB7B-JM6p",
        "TQSGZKVRNKx2",
        "xVqTAB8JJOcj",
        "-6fTaB7mNInb",
        "cvNUjt_cJQA_",
        "g1m4XORqNGPM",
        "0TYJKz4YJRxv",
        "O8MFxw5SZwyn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
